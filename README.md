# Ollama-using-Cuda
A local Ollama llm using Cuda, Streamlit and Ngrok.
